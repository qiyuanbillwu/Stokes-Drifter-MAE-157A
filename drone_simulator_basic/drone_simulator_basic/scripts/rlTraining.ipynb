{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c56e46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import TD3\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.noise import NormalActionNoise\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e1b822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to ./logs\\TD3_10\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 526      |\n",
      "|    ep_rew_mean     | 241      |\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 1051     |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2106     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 161      |\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 1091     |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 4586     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 538      |\n",
      "|    ep_rew_mean     | 168      |\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 823      |\n",
      "|    time_elapsed    | 7        |\n",
      "|    total_timesteps | 6457     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 572      |\n",
      "|    ep_rew_mean     | 176      |\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 883      |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 9153     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 10000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 608      |\n",
      "|    ep_rew_mean     | 129      |\n",
      "| time/              |          |\n",
      "|    episodes        | 20       |\n",
      "|    fps             | 820      |\n",
      "|    time_elapsed    | 14       |\n",
      "|    total_timesteps | 12170    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | 78.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 24       |\n",
      "|    fps             | 853      |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 14709    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 15000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 606      |\n",
      "|    ep_rew_mean     | 81.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 28       |\n",
      "|    fps             | 805      |\n",
      "|    time_elapsed    | 21       |\n",
      "|    total_timesteps | 16959    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 605      |\n",
      "|    ep_rew_mean     | 94.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 32       |\n",
      "|    fps             | 832      |\n",
      "|    time_elapsed    | 23       |\n",
      "|    total_timesteps | 19349    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 20000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 609      |\n",
      "|    ep_rew_mean     | 94.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 36       |\n",
      "|    fps             | 803      |\n",
      "|    time_elapsed    | 27       |\n",
      "|    total_timesteps | 21934    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 613      |\n",
      "|    ep_rew_mean     | 89.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 40       |\n",
      "|    fps             | 826      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 24532    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 25000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 606      |\n",
      "|    ep_rew_mean     | 89.2     |\n",
      "| time/              |          |\n",
      "|    episodes        | 44       |\n",
      "|    fps             | 797      |\n",
      "|    time_elapsed    | 33       |\n",
      "|    total_timesteps | 26663    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 615      |\n",
      "|    ep_rew_mean     | 81.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 48       |\n",
      "|    fps             | 820      |\n",
      "|    time_elapsed    | 36       |\n",
      "|    total_timesteps | 29533    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 30000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 604      |\n",
      "|    ep_rew_mean     | 84.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 52       |\n",
      "|    fps             | 798      |\n",
      "|    time_elapsed    | 39       |\n",
      "|    total_timesteps | 31407    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 589      |\n",
      "|    ep_rew_mean     | 92.4     |\n",
      "| time/              |          |\n",
      "|    episodes        | 56       |\n",
      "|    fps             | 809      |\n",
      "|    time_elapsed    | 40       |\n",
      "|    total_timesteps | 32971    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 580      |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    episodes        | 60       |\n",
      "|    fps             | 821      |\n",
      "|    time_elapsed    | 42       |\n",
      "|    total_timesteps | 34775    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 35000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 578      |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    episodes        | 64       |\n",
      "|    fps             | 802      |\n",
      "|    time_elapsed    | 46       |\n",
      "|    total_timesteps | 36973    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 584      |\n",
      "|    ep_rew_mean     | 110      |\n",
      "| time/              |          |\n",
      "|    episodes        | 68       |\n",
      "|    fps             | 818      |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 39722    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 40000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 583      |\n",
      "|    ep_rew_mean     | 109      |\n",
      "| time/              |          |\n",
      "|    episodes        | 72       |\n",
      "|    fps             | 803      |\n",
      "|    time_elapsed    | 52       |\n",
      "|    total_timesteps | 42006    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 573      |\n",
      "|    ep_rew_mean     | 112      |\n",
      "| time/              |          |\n",
      "|    episodes        | 76       |\n",
      "|    fps             | 810      |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 43547    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 45000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 576      |\n",
      "|    ep_rew_mean     | 97.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 80       |\n",
      "|    fps             | 797      |\n",
      "|    time_elapsed    | 57       |\n",
      "|    total_timesteps | 46093    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 570      |\n",
      "|    ep_rew_mean     | 98.5     |\n",
      "| time/              |          |\n",
      "|    episodes        | 84       |\n",
      "|    fps             | 805      |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 47878    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 568      |\n",
      "|    ep_rew_mean     | 97.8     |\n",
      "| time/              |          |\n",
      "|    episodes        | 88       |\n",
      "|    fps             | 814      |\n",
      "|    time_elapsed    | 61       |\n",
      "|    total_timesteps | 49956    |\n",
      "---------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=210.78 +/- 0.00\n",
      "Episode length: 362.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 362      |\n",
      "|    mean_reward     | 211      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 50000    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 552      |\n",
      "|    ep_rew_mean     | 98       |\n",
      "| time/              |          |\n",
      "|    episodes        | 92       |\n",
      "|    fps             | 610      |\n",
      "|    time_elapsed    | 83       |\n",
      "|    total_timesteps | 50815    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.389   |\n",
      "|    critic_loss     | 0.223    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 814      |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 544      |\n",
      "|    ep_rew_mean     | 99.9     |\n",
      "| time/              |          |\n",
      "|    episodes        | 96       |\n",
      "|    fps             | 444      |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 52244    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -0.994   |\n",
      "|    critic_loss     | 0.643    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 2243     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 533      |\n",
      "|    ep_rew_mean     | 101      |\n",
      "| time/              |          |\n",
      "|    episodes        | 100      |\n",
      "|    fps             | 374      |\n",
      "|    time_elapsed    | 142      |\n",
      "|    total_timesteps | 53271    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.8     |\n",
      "|    critic_loss     | 17.3     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 3270     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 520      |\n",
      "|    ep_rew_mean     | 95.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 104      |\n",
      "|    fps             | 335      |\n",
      "|    time_elapsed    | 160      |\n",
      "|    total_timesteps | 54070    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -1.8     |\n",
      "|    critic_loss     | 1        |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4069     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 504      |\n",
      "|    ep_rew_mean     | 96.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 108      |\n",
      "|    fps             | 302      |\n",
      "|    time_elapsed    | 181      |\n",
      "|    total_timesteps | 54957    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.6     |\n",
      "|    critic_loss     | 2.71     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4956     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=104.88 +/- 0.00\n",
      "Episode length: 225.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 225      |\n",
      "|    mean_reward     | 105      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 55000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.7     |\n",
      "|    critic_loss     | 0.273    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 4999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 494      |\n",
      "|    ep_rew_mean     | 92.7     |\n",
      "| time/              |          |\n",
      "|    episodes        | 112      |\n",
      "|    fps             | 273      |\n",
      "|    time_elapsed    | 204      |\n",
      "|    total_timesteps | 55814    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.75    |\n",
      "|    critic_loss     | 0.493    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 5813     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 475      |\n",
      "|    ep_rew_mean     | 88.1     |\n",
      "| time/              |          |\n",
      "|    episodes        | 116      |\n",
      "|    fps             | 252      |\n",
      "|    time_elapsed    | 224      |\n",
      "|    total_timesteps | 56620    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -2.56    |\n",
      "|    critic_loss     | 3.81     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 6619     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 452      |\n",
      "|    ep_rew_mean     | 94       |\n",
      "| time/              |          |\n",
      "|    episodes        | 120      |\n",
      "|    fps             | 235      |\n",
      "|    time_elapsed    | 243      |\n",
      "|    total_timesteps | 57406    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.11    |\n",
      "|    critic_loss     | 0.727    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 7405     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 435      |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 124      |\n",
      "|    fps             | 221      |\n",
      "|    time_elapsed    | 263      |\n",
      "|    total_timesteps | 58236    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -3.86    |\n",
      "|    critic_loss     | 22       |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 8235     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 422      |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 128      |\n",
      "|    fps             | 206      |\n",
      "|    time_elapsed    | 285      |\n",
      "|    total_timesteps | 59130    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.79    |\n",
      "|    critic_loss     | 1.18     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9129     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=138.74 +/- 0.00\n",
      "Episode length: 250.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 250      |\n",
      "|    mean_reward     | 139      |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 60000    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -5.95    |\n",
      "|    critic_loss     | 1.46     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 9999     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 408      |\n",
      "|    ep_rew_mean     | 104      |\n",
      "| time/              |          |\n",
      "|    episodes        | 132      |\n",
      "|    fps             | 191      |\n",
      "|    time_elapsed    | 313      |\n",
      "|    total_timesteps | 60199    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -4.56    |\n",
      "|    critic_loss     | 6.08     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 10198    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 392      |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    episodes        | 136      |\n",
      "|    fps             | 181      |\n",
      "|    time_elapsed    | 336      |\n",
      "|    total_timesteps | 61152    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -8.61    |\n",
      "|    critic_loss     | 0.928    |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11151    |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 375      |\n",
      "|    ep_rew_mean     | 107      |\n",
      "| time/              |          |\n",
      "|    episodes        | 140      |\n",
      "|    fps             | 173      |\n",
      "|    time_elapsed    | 357      |\n",
      "|    total_timesteps | 62028    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | -6.52    |\n",
      "|    critic_loss     | 1.83     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 12027    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal evaluation reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmean_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ± \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd_reward\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 48\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m TD3(\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     33\u001b[0m     env,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Save final model\u001b[39;00m\n\u001b[0;32m     51\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(log_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\ryan\\anaconda3\\envs\\py396_env\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:222\u001b[0m, in \u001b[0;36mTD3.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfTD3,\n\u001b[0;32m    215\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    221\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfTD3:\n\u001b[1;32m--> 222\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan\\anaconda3\\envs\\py396_env\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py:347\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[38;5;66;03m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m gradient_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 347\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgradient_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    349\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\ryan\\anaconda3\\envs\\py396_env\\lib\\site-packages\\stable_baselines3\\td3\\td3.py:188\u001b[0m, in \u001b[0;36mTD3.train\u001b[1;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;66;03m# Optimize the critics\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 188\u001b[0m \u001b[43mcritic_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcritic\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    191\u001b[0m \u001b[38;5;66;03m# Delayed policy updates\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryan\\anaconda3\\envs\\py396_env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ryan\\anaconda3\\envs\\py396_env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import custom environment\n",
    "from drone_env import DroneEnv  # update path as needed\n",
    "\n",
    "def make_env():\n",
    "    return Monitor(DroneEnv())  # Wrap with Monitor for logging\n",
    "\n",
    "def main():\n",
    "    log_dir = \"./logs\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Create training environment\n",
    "    env = make_vec_env(make_env, n_envs=1)\n",
    "\n",
    "    # Evaluation environment\n",
    "    eval_env = make_env()\n",
    "\n",
    "    n_actions = env.action_space.shape[-1]\n",
    "    action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.2 * np.ones(n_actions))\n",
    "\n",
    "    # Callback to save best model\n",
    "    eval_callback = EvalCallback(\n",
    "        eval_env,\n",
    "        best_model_save_path=log_dir + \"/best_model\",\n",
    "        log_path=log_dir,\n",
    "        eval_freq=5000,\n",
    "        deterministic=True,\n",
    "        render=False\n",
    "    )\n",
    "\n",
    "    # Create the model\n",
    "    model = TD3(\n",
    "        \"MlpPolicy\",\n",
    "        env,\n",
    "        verbose=1,\n",
    "        learning_rate=0.001,\n",
    "        batch_size=256,\n",
    "        buffer_size=100_000,\n",
    "        learning_starts=50_000,\n",
    "        train_freq=(1, \"step\"),\n",
    "        tau=0.005,\n",
    "        gamma=0.99,\n",
    "        tensorboard_log=log_dir,\n",
    "        action_noise=action_noise\n",
    "\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.learn(total_timesteps=600_000, callback=eval_callback)\n",
    "\n",
    "    # Save final model\n",
    "    model.save(os.path.join(log_dir, \"final_model\"))\n",
    "\n",
    "    print(\"Training complete. Model saved.\")\n",
    "\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10)\n",
    "    print(f\"Final evaluation reward: {mean_reward:.2f} ± {std_reward:.2f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py396_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
